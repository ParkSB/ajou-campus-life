minkowski distance → 유클리드 거리와 맨해튼 거리를 일반화한 거리. r이 1일 때는 맨해튼 거리, 2일 때는 유클리드 거리가 된다. 무한대인 경우 최대 거리가 된다.

Simple Matching Coefficient (SMC) → 전체 케이스 중 일치하는 케이스의 비율로 표현한 유사도 지표.

Jaccard Coefficient → SMC에서 둘 다 0인 케이스를 제외한 유사도 지표.

Support Vector Machine (SVM) → sum of max(0, s_j - s_y + 1) 형태의 손실 함수.

Backpropagation → 연산 결과와 정답의 오차를 구해 이 오차에 관여하는 값들의 가중치를 수정해 오차가 작아지는 방향으로 반복해 모델을 개선하는 것.

Stride → 필터의 보폭을 조정하는 기법. 입력의 크기가 줄어든다.

Padding → border를 0으로 채워 입력의 크기가 줄어드는 것을 방지하는 기법.

Pooling → 입력 데이터의 크기를 줄이기 위한 기법. 필터 영역에서 특정 기준으로 일부 값을 골라 출력을 만든다.

Output activation map size → (N + 2P - F / stride) + 1

7x7 input, 3x3 filter, then output activation map size? → 5

7x7 input, 3x3 filter, stride 2, then output activation map size? → 3

7x7 input, 3x3 filter, pad 1, then output activation map size? → 7

32x32x3 input, 10 5x5 filters, pad 2, then output activation map volume? → 32x32x10

32x32x3 input, 10 5x5 filters, pad 2, then the number of parameters? → 760

YOLO, SDD의 장단점은? → regional proposal 계산을 하지 않기 때문에 빠르다. 하지만 정확도가 떨어진다.

Sementic segmentation → 이미지의 각 픽셀이 어떤 카테고리에 속하는지 분류함으로써 영역을 분리하는 것.

Downsampling → pooling, strided convolution

Upsampling → unpooling

bed of nails와 max unpooling의 차이는? → max unpooing은 각 영역의 max 위치를 기억해두고 그 위치에 값을 복원한다. 반면 bed of nails는 모든 영역에서 같은 위치에 값을 복원한다.

Transpose convolution → feature map을 unsampling 할 때 어떤 식으로 할지 학습하는 방식.

좋은 learning rate는? → 처음에는 high, epoch가 늘어남에 따라 low로 조정하는 것이 좋다.

Dropout → 학습 과정에서 랜덤하게 일부 노드를 제거하는 것. 오버피팅을 개선하기 위한 방안.

LeNet → 성공적으로 적용된 최초의 ConvNet.

AlexNet → 최초의 large scale CNN.

ZFNet → AlexNet의 하이퍼 파라미터를 조정해 개선한 아키텍처.

VGGNet → 작고 깊은 네트워크. 파라미터 수가 줄어들지만 더 깊음.

GoogleNet (Inception) → 동일한 입력을 받는 서로 다른 다양한 필터들이 병렬적으로 연결된 네트워크.

ResNet → 깊이가 대폭 깊어진 아키텍처. 이전 레이어에서 흘러운 입력을 출력에 더하는 skip connection. 최종 출력은 입력에 찬자를 더한 x + F(x)가 된다.

Regions of Interest (ROI) → 이미지에서 추출하고자하는 영역.

Region proposal → 크기가 제각각인 ROI의 크기를 동일하게 변환하는 것.

Faster-RNN → Region proposal을 개선해 빠른 R-CNN. 모든 proposals가 같은 feature map을 공유한다.

비지도학습 → 데이터는 있지만 레이블은 없는 상태에서 데이터의 숨겨진 구조를 파악하는 학습. K-means 등 클러스터링.

Segmentation → 이미지 내에서 영역을 분리하는 것.

argmax_x y → y가 최대가 되게 만드는 x를 구하는 함수

Variational Auto Encoders → 입력 이미지의 대표 features를 추출해 z vector에 담고, 이를 통해 유사한 데이터를 생성하는 모델. intractable density function을 정의해 사용하므로 직접 최적화가 불가능.

Generative Adversarial Networks (GAN) → 명시적인 밀도 함수가 아니라 게임 이론 기반의 학습 분포로 생성을 학습하는 생성 모델. 단, 안정적이지 않은 학습, 추론 쿼리가 없음.

지도학습 → 데이터 x와 레이블 y를 바탕으로 x를 y에 매핑하는 함수를 학습하는 것.

강화학습 → 에이전트와 환경이 주어졌을 때 에이전트가 보상을 최대화하는 방향으로 환경을 변경하는 행동을 결정하도록 학습하는 것.

Markov Decision Process (MDP) → 의사결정 과정을 모델링하는 수학적 프레임워크로, 이를 통해 강화학습을 수식화할 수 있음.

Markov property: S → 가능한 상태의 집합

Markov property: A → 가능한 행동의 집합

Markov property: R → 상태와 행동의 페어에 대한 보상의 분포.

Markov property: P → transition probaility. 상태와 행동 페어가 주어졌을 때 전이될 다음 상태에 대한 분포.

Markov property: gamma → discount factor. 시간에 따라 보상을 얼마나 할인할지 결정하는 인자. 현재에 가까운 보상일수록 큰 가치를 갖게 만든다.

Policy pi → 주어진 상태에서 에이전트가 어떤 행동을 취할지에 대한 전략.

Value function → 상태 s와 정책 pi가 주어졌을 때 상태가 얼마나 가치있는지 정의하는 함수.

Q-value function → 상태 s와 정책 pi, 행동 a가 주어졌을 때 행동이 얼마나 가치있는지 정의하는 함수.

Bellman equation → 현재 state value와 미래의 state value의 관계를 나타내는 방정식. s'는 에피소드가 끝나는 시점의 상태, 따라서 Q*(s, a)는 현재 상태, 액션 페어로 받을 수 있는 리워드와 에피소드가 끝나는 시점까지의 보상을 합한 값의 기대값.

Value iteration → Q의 최적화를 위해 bellman equation을 이용해 iterative update하는 기법. i가 무한대로 갈 때 최적의 Q*로 수렴. 모든 페어에 대해 연산을 해야 하므로 시간이 오래 걸림.

Q-learning → action value 함수를 추정하기 위한 근사 함수를 사용하는 기법. Q는 최적의 Q인 Q*을 근사한다. 순전파에서는 벨만 방정식의 loss가 최소화되도록 q-function을 학습하고, 역전파에서는 손실을 기반으로 파라미터 theta를 업데이트한다.

Experience replay → 샘플에 상관관계가 있다면 계속 이전 정책만 최적의 정책으로 선택할 수 있음. 이를 방지하기 위해 replay memory에 transitive table을 두고 임의로 샘플링.

Policy gradients → 정책 자체를 학습해 최적의 정책을 찾는 기법. J(theta)는 정책에 대한 파라미터 theta를 이용해 pi_theta가 주어졌을 때 cumulative reward의 기대값을 정의한다. 즉, theta* = argmax_theta J(theta)

Policy gradients baseline → 선택해온 상태와 액션의 페어가 모두 의미있는건 아님. 얻은 보상이 예상보다 좋은지만 판단하면 된다. 이때 베이스라인을 정해두는 것.

Fast Gradient Sign Method (FGSM) → 사람 눈에 보이지 않는 노이즈 레이어를 이미지 위에 덮어 인공지능 모델의 판단을 교란하는 공격 기법.

noise와 outlier의 차이 → noise는 정상적이지 않은 값. outlier는 평균치에서 크게 벗어난 값. noise와 달리 outlier는 주목해서 볼 필요가 있음.

Accuracy → TP + TN / TP + FN + FP + TN

Error rate → 1 - Accuracy

Precision → TP / TP + FP

Recall → TP / TP + FN

F-measure → 2TP / 2TP + FN + FP

TN Rate → TN / FP + TN

FP Rate → 1 - TN Rate

FN Rate → 1 - Recall

Association rule → 두 데이터 사이의 상호 연관된 규칙성. 가령 우유와 기저귀를 사면, 맥주를 산다. ({a, b} => {c})

Support count (sigma) → 아이템셋의 출현 빈도

Support (s) → 해당 아이템셋을 가진 트랜잭션의 비율 (sigma(a, b, c) / |T|)

Confidence → X를 가진 트랜잭션에서의 Y에 대한 비율 (sigma(a, b, c) / sigma(a, b))

Minsup → 최소한의 support. minsup 미만의 아이템셋의 출현은 유의미하지 않은 것으로 간주하고 소거.

K-Means → 클러스터링 알고리즘. 군집이 원형이 아닌 경우 문제가 생길수도.

DBSCAN → 클러스터링 알고리즘. K-Means와 달리 군집이 원형이 아닌 경우에도 클러스터링이 잘 됨. epsilon은 이웃을 정의하는 거리.
